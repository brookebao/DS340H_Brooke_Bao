results_long <- results %>%
pivot_longer(cols = c("low_accuracy", "medium_accuracy", "overall_accuracy"),
names_to = "Metric", values_to = "Accuracy")
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Low Class Accuracy vs. Overall Accuracy",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
theme_minimal()
thresholds <- seq(0.5, 1, by = 0.05)
results <- data.frame(
threshold = numeric(),
low_accuracy = numeric(),
medium_accuracy = numeric(),
overall_accuracy = numeric()
)
for (t in thresholds) {
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
#counts in the table
low_low <- cm["Low", "Low"]
medium_low <- cm["Medium", "Low"]
medium_medium <- cm["Medium", "Medium"]
low_medium <- cm["Low", "Medium"]
#calculating number of lows, highs, actual vs predicted
actual_low_total <- low_low + medium_low
correct_total <- low_low + medium_medium
actual_medium_total <- low_medium + medium_medium
total_rows <- nrow(df)
#if the total is 0 put NA so won't divide by 0
results <- rbind(results, data.frame(
Threshold = t,
low_accuracy = ifelse(actual_low_total == 0, NA, low_low/actual_low_total),
medium_accuracy = ifelse(actual_medium_total == 0, NA, medium_medium/actual_medium_total),
overall_accuracy = correct_total / total_rows
))
}
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
#table(Y.hat.70)
#tail(p.hat)
cm <- table(Predicted = Y.hat.prop, Actual = df$cluster) #confusion matrix
cm
t = 0.75
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
cm <- table(df$actual, y.hat)  #confusion matrix
View(df)
cm <- table(df$cluster, y.hat)  #confusion matrix
t = 0.75
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
cm <- table(df$cluster, y.hat)  #confusion matrix
cm
#counts in the table
low_low <- cm["Low", "Low"]
low_low
medium_low <- cm["Medium", "Low"]
medium_low
#table(Y.hat.70)
#tail(p.hat)
table(Predicted = Y.hat.prop, Actual = df$cluster) #confusion matrix
#table(Y.hat.70)
#tail(p.hat)
table(Actual = df$cluster, Predicted = Y.hat.prop) #confusion matrix
#table(Y.hat.70)
#tail(p.hat)
table(Actual = df$cluster, Predicted = Y.hat.prop) #confusion matrix
#table(Y.hat.70)
#tail(p.hat)
table(Predicted = Y.hat.prop, Actual = df$cluster) #confusion matrix
#table(Y.hat.70)
#tail(p.hat)
table(Predicted = Y.hat.prop, Actual = df$cluster) #confusion matrix
t = 0.75
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
cm <- table(df$cluster, y.hat)  #confusion matrix
cm
t = 0.75
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
cm <- table(y.hat, df$cluster)  #confusion matrix
cm
#counts in the table, predicted-actual
low_low <- cm["Low", "Low"]
low_low
medium_low <- cm["Medium", "Low"]
medium_low
thresholds <- seq(0.5, 1, by = 0.05)
results <- data.frame(
threshold = numeric(),
low_accuracy = numeric(),
medium_accuracy = numeric(),
overall_accuracy = numeric()
)
for (t in thresholds) {
t = 0.75
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
cm <- table(y.hat, df$cluster)  #confusion matrix
#counts in the table, predicted-actual
low_low <- cm["Low", "Low"]
medium_low <- cm["Medium", "Low"]
medium_medium <- cm["Medium", "Medium"]
low_medium <- cm["Low", "Medium"]
#calculating number of lows, highs, actual vs predicted
actual_low_total <- low_low + medium_low
correct_total <- low_low + medium_medium
actual_medium_total <- low_medium + medium_medium
total_rows <- nrow(df)
#if the total is 0 put NA so won't divide by 0
results <- rbind(results, data.frame(
Threshold = t,
low_accuracy = ifelse(actual_low_total == 0, NA, low_low/actual_low_total),
medium_accuracy = ifelse(actual_medium_total == 0, NA, medium_medium/actual_medium_total),
overall_accuracy = correct_total / total_rows
))
}
results_long <- results %>%
pivot_longer(cols = c("low_accuracy", "medium_accuracy", "overall_accuracy"),
names_to = "Metric", values_to = "Accuracy")
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Low Class Accuracy vs. Overall Accuracy",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
theme_minimal()
thresholds <- seq(0.5, 1, by = 0.05)
results <- data.frame(
threshold = numeric(),
low_accuracy = numeric(),
medium_accuracy = numeric(),
overall_accuracy = numeric()
)
for (t in thresholds) {
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
cm <- table(y.hat, df$cluster)  #confusion matrix
#counts in the table, predicted-actual
low_low <- cm["Low", "Low"]
medium_low <- cm["Medium", "Low"]
medium_medium <- cm["Medium", "Medium"]
low_medium <- cm["Low", "Medium"]
#calculating number of lows, highs, actual vs predicted
actual_low_total <- low_low + medium_low
correct_total <- low_low + medium_medium
actual_medium_total <- low_medium + medium_medium
total_rows <- nrow(df)
#if the total is 0 put NA so won't divide by 0
results <- rbind(results, data.frame(
Threshold = t,
low_accuracy = ifelse(actual_low_total == 0, NA, low_low/actual_low_total),
medium_accuracy = ifelse(actual_medium_total == 0, NA, medium_medium/actual_medium_total),
overall_accuracy = correct_total / total_rows
))
}
results_long <- results %>%
pivot_longer(cols = c("low_accuracy", "medium_accuracy", "overall_accuracy"),
names_to = "Metric", values_to = "Accuracy")
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Low Class Accuracy vs. Overall Accuracy",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
theme_minimal()
thresholds <- seq(0.5, 1, by = 0.05)
results <- data.frame(
threshold = numeric(),
low_accuracy = numeric(),
medium_accuracy = numeric(),
overall_accuracy = numeric()
)
for (t in thresholds) {
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
cm <- table(y.hat, df$cluster)  #confusion matrix
#counts in the table, predicted-actual
low_low <- cm["Low", "Low"]
medium_low <- cm["Medium", "Low"]
medium_medium <- cm["Medium", "Medium"]
low_medium <- cm["Low", "Medium"]
#calculating number of lows, highs, actual vs predicted
actual_low_total <- low_low + medium_low
correct_total <- low_low + medium_medium
actual_medium_total <- low_medium + medium_medium
total_rows <- nrow(df)
#if the total is 0 put NA so won't divide by 0
results <- rbind(results, data.frame(
Threshold = t,
low_accuracy = ifelse(actual_low_total == 0, NA, low_low/actual_low_total),
medium_accuracy = ifelse(actual_medium_total == 0, NA, medium_medium/actual_medium_total),
overall_accuracy = correct_total / total_rows
))
}
#counts in the table, predicted-actual
low_low <- cm["Low", "Low"]
thresholds <- seq(0.5, 1, by = 0.05)
results <- data.frame(
threshold = numeric(),
low_accuracy = numeric(),
medium_accuracy = numeric(),
overall_accuracy = numeric()
)
for (t in thresholds) {
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
cm <- table(y.hat, df$cluster)  #confusion matrix
print(cm)
#counts in the table, predicted-actual
low_low <- cm["Low", "Low"]
medium_low <- cm["Medium", "Low"]
medium_medium <- cm["Medium", "Medium"]
low_medium <- cm["Low", "Medium"]
#calculating number of lows, highs, actual vs predicted
actual_low_total <- low_low + medium_low
correct_total <- low_low + medium_medium
actual_medium_total <- low_medium + medium_medium
total_rows <- nrow(df)
#if the total is 0 put NA so won't divide by 0
results <- rbind(results, data.frame(
Threshold = t,
low_accuracy = ifelse(actual_low_total == 0, NA, low_low/actual_low_total),
medium_accuracy = ifelse(actual_medium_total == 0, NA, medium_medium/actual_medium_total),
overall_accuracy = correct_total / total_rows
))
}
#counts in the table, predicted-actual
#low_low <- cm["Low", "Low"]
# medium_low <- cm["Medium", "Low"]
# medium_medium <- cm["Medium", "Medium"]
#low_medium <- cm["Low", "Medium"]
#Sometimes it will predict nothign so need to put 0
low_low <- if ("Low" %in% rownames(cm) && "Low" %in% colnames(cm)) cm["Low", "Low"] else 0
medium_low <- if ("Medium" %in% rownames(cm) && "Low" %in% colnames(cm)) cm["Medium", "Low"] else 0
thresholds <- seq(0.5, 1, by = 0.05)
results <- data.frame(
threshold = numeric(),
low_accuracy = numeric(),
medium_accuracy = numeric(),
overall_accuracy = numeric()
)
for (t in thresholds) {
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
cm <- table(y.hat, df$cluster)  #confusion matrix
print(cm)
#counts in the table, predicted-actual
#low_low <- cm["Low", "Low"]
# medium_low <- cm["Medium", "Low"]
# medium_medium <- cm["Medium", "Medium"]
#low_medium <- cm["Low", "Medium"]
#Sometimes it will predict nothign so need to put 0
low_low <- if ("Low" %in% rownames(cm) && "Low" %in% colnames(cm)) cm["Low", "Low"] else 0
medium_low <- if ("Medium" %in% rownames(cm) && "Low" %in% colnames(cm)) cm["Medium", "Low"] else 0
medium_medium <- if ("Medium" %in% rownames(cm) && "Medium" %in% colnames(cm)) cm["Medium", "Medium"] else 0
low_medium <- if ("Low" %in% rownames(cm) && "Medium" %in% colnames(cm)) cm["Low", "Medium"] else 0
#calculating number of lows, highs, actual vs predicted
actual_low_total <- low_low + medium_low
correct_total <- low_low + medium_medium
actual_medium_total <- low_medium + medium_medium
total_rows <- nrow(df)
#if the total is 0 put NA so won't divide by 0
results <- rbind(results, data.frame(
Threshold = t,
low_accuracy = ifelse(actual_low_total == 0, NA, low_low/actual_low_total),
medium_accuracy = ifelse(actual_medium_total == 0, NA, medium_medium/actual_medium_total),
overall_accuracy = correct_total / total_rows
))
}
results_long <- results %>%
pivot_longer(cols = c("low_accuracy", "medium_accuracy", "overall_accuracy"),
names_to = "Metric", values_to = "Accuracy")
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Low Class Accuracy vs. Overall Accuracy",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
theme_minimal()
View(df)
thresholds <- seq(0.5, 1, by = 0.05)
results <- data.frame(
threshold = numeric(),
low_accuracy = numeric(),
medium_accuracy = numeric(),
overall_accuracy = numeric()
)
for (t in thresholds) {
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
cm <- table(y.hat, df$cluster)  #confusion matrix
print(cm)
#counts in the table, predicted-actual
#low_low <- cm["Low", "Low"]
# medium_low <- cm["Medium", "Low"]
# medium_medium <- cm["Medium", "Medium"]
#low_medium <- cm["Low", "Medium"]
#Sometimes it will predict nothign so need to put 0
low_low <- if ("Low" %in% rownames(cm) && "Low" %in% colnames(cm)) cm["Low", "Low"] else 0
medium_low <- if ("Medium" %in% rownames(cm) && "Low" %in% colnames(cm)) cm["Medium", "Low"] else 0
medium_medium <- if ("Medium" %in% rownames(cm) && "Medium" %in% colnames(cm)) cm["Medium", "Medium"] else 0
low_medium <- if ("Low" %in% rownames(cm) && "Medium" %in% colnames(cm)) cm["Low", "Medium"] else 0
#calculating number of lows, highs, actual vs predicted
actual_low_total <- low_low + medium_low
correct_total <- low_low + medium_medium
actual_medium_total <- low_medium + medium_medium
total_rows <- nrow(df)
#if the total is 0 put NA so won't divide by 0
results <- rbind(results, data.frame(
Threshold = t,
low_accuracy = ifelse(actual_low_total == 0, NA, low_low/actual_low_total),
medium_accuracy = ifelse(actual_medium_total == 0, NA, medium_medium/actual_medium_total),
overall_accuracy = correct_total / total_rows
))
}
results_long <- results %>%
pivot_longer(cols = c("low_accuracy", "medium_accuracy", "overall_accuracy"),
names_to = "Metric", values_to = "Accuracy")
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Low Class Accuracy vs. Overall Accuracy",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
theme_minimal()
thresholds <- seq(0.5, 1, by = 0.05)
results <- data.frame(
threshold = numeric(),
low_accuracy = numeric(),
medium_accuracy = numeric(),
overall_accuracy = numeric()
)
for (t in thresholds) {
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
cm <- table(y.hat, df$cluster)  #confusion matrix
print(cm)
#counts in the table, predicted-actual
low_low <- cm["Low", "Low"]
medium_low <- cm["Medium", "Low"]
medium_medium <- cm["Medium", "Medium"]
low_medium <- cm["Low", "Medium"]
#Sometimes it will predict nothign so need to put 0
#calculating number of lows, highs, actual vs predicted
actual_low_total <- low_low + medium_low
correct_total <- low_low + medium_medium
actual_medium_total <- low_medium + medium_medium
total_rows <- nrow(df)
#if the total is 0 put NA so won't divide by 0
results <- rbind(results, data.frame(
Threshold = t,
low_accuracy = ifelse(actual_low_total == 0, NA, low_low/actual_low_total),
medium_accuracy = ifelse(actual_medium_total == 0, NA, medium_medium/actual_medium_total),
overall_accuracy = correct_total / total_rows
))
}
results_long <- results %>%
pivot_longer(cols = c("low_accuracy", "medium_accuracy", "overall_accuracy"),
names_to = "Metric", values_to = "Accuracy")
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Low Class Accuracy vs. Overall Accuracy",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
theme_minimal()
thresholds <- seq(0.5, 0.99, by = 0.05)
results <- data.frame(
threshold = numeric(),
low_accuracy = numeric(),
medium_accuracy = numeric(),
overall_accuracy = numeric()
)
for (t in thresholds) {
y.hat <- ifelse(p.hat[, "Medium"] > t, "Medium", "Low")
cm <- table(y.hat, df$cluster)  #confusion matrix
print(cm)
#counts in the table, predicted-actual
#low_low <- cm["Low", "Low"]
# medium_low <- cm["Medium", "Low"]
# medium_medium <- cm["Medium", "Medium"]
#low_medium <- cm["Low", "Medium"]
#Sometimes it will predict nothign so need to put 0
low_low <- if ("Low" %in% rownames(cm) && "Low" %in% colnames(cm)) cm["Low", "Low"] else 0
medium_low <- if ("Medium" %in% rownames(cm) && "Low" %in% colnames(cm)) cm["Medium", "Low"] else 0
medium_medium <- if ("Medium" %in% rownames(cm) && "Medium" %in% colnames(cm)) cm["Medium", "Medium"] else 0
low_medium <- if ("Low" %in% rownames(cm) && "Medium" %in% colnames(cm)) cm["Low", "Medium"] else 0
#calculating number of lows, highs, actual vs predicted
actual_low_total <- low_low + medium_low
correct_total <- low_low + medium_medium
actual_medium_total <- low_medium + medium_medium
total_rows <- nrow(df)
#if the total is 0 put NA so won't divide by 0
results <- rbind(results, data.frame(
Threshold = t,
low_accuracy = ifelse(actual_low_total == 0, NA, low_low/actual_low_total),
medium_accuracy = ifelse(actual_medium_total == 0, NA, medium_medium/actual_medium_total),
overall_accuracy = correct_total / total_rows
))
}
results_long <- results %>%
pivot_longer(cols = c("low_accuracy", "medium_accuracy", "overall_accuracy"),
names_to = "Metric", values_to = "Accuracy")
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Low Class Accuracy vs. Overall Accuracy",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
theme_minimal()
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Low Class Accuracy vs. Overall Accuracy",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
labels = c(
"low_accuracy" = "Low Proportion Correct",
"medium_accuracy" = "Medium Proportion Correct",
"overall_accuracy" = "Overall Accuracy"
))
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Low Class Accuracy vs. Overall Accuracy",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
labels = c(
"low_accuracy" = "Low Proportion Correct",
"medium_accuracy" = "Medium Proportion Correct",
"overall_accuracy" = "Overall Accuracy"
)
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Low Class Accuracy vs. Overall Accuracy",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
labels = c(
"low_accuracy" = "Low Proportion Correct",
"medium_accuracy" = "Medium Proportion Correct",
"overall_accuracy" = "Overall Accuracy"
)+
theme_minimal()
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Low Class Accuracy vs. Overall Accuracy",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
scale_color_manual(
values = c(
"low_accuracy" = "#1b9e77",
"medium_accuracy" = "#d95f02",
"overall_accuracy" = "#7570b3"
),
labels = c(
"low_accuracy" = "Low Proportion Correct",
"medium_accuracy" = "Medium Proportion Correct",
"overall_accuracy" = "Overall Accuracy"
))+
theme_minimal()
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Accuracy for Different Thresholds",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
scale_color_manual( #need this to relable the legend...
values = c(
"low_accuracy" = "#1b9e77",
"medium_accuracy" = "#d95f02",
"overall_accuracy" = "#7570b3"
),
labels = c(
"low_accuracy" = "Low Flow \n Proportion Correct",
"medium_accuracy" = "Medium Flow \nProportion Correct",
"overall_accuracy" = "Overall Accuracy"
))+
theme_minimal()
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Accuracy for Different Thresholds",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
scale_color_manual( #need this to relable the legend...
values = c(
"low_accuracy" = "#1b9e77",
"medium_accuracy" = "#d95f02",
"overall_accuracy" = "#7570b3"
),
labels = c(
"low_accuracy" = "Low Flow \n Proportion Correct\n",
"medium_accuracy" = "Medium Flow \nProportion Correct",
"overall_accuracy" = "Overall Accuracy"
))+
theme_minimal()
ggplot(results_long, aes(x = Threshold, y = Accuracy, color = Metric)) +
geom_line(size = 1) +
labs(title = "Accuracy for Different Thresholds",
x = "Threshold for Predicting 'Medium'",
y = "Proportion Correct",
color = "Metric") +
scale_color_manual( #need this to relable the legend...
values = c(
"low_accuracy" = "#1b9e77",
"medium_accuracy" = "#d95f02",
"overall_accuracy" = "#7570b3"
),
labels = c(
"low_accuracy" = "Low-Flow \n Proportion Correct\n",
"medium_accuracy" = "Medium-Flow \n Proportion Correct\n",
"overall_accuracy" = "Overall Accuracy"
))+
theme_minimal()
#varImpPlot(randomForestModel, main = "Variable Importance for RF with Average Net Flow")
varImpPlot(randomForestModel1, main = "Variable Importance from Random Forest without Net Flow")
#varImpPlot(randomForestModel, main = "Variable Importance for RF with Average Net Flow")
#print only mean decrease accuracy
varImpPlot(randomForestModel1, type = 1, main = "Variable Importance from Random Forest")
